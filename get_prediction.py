# -*- coding: utf-8 -*-
"""get_prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1wwQjSFRPR2UzHkxyzIJXjS7T8TnEVDnL
"""


!pip install transformers



import torch
import streamlit as st
import matplotlib.pyplot as plt
from PIL import Image
from torchvision import transforms, models
from transformers import AutoTokenizer, AutoModel, AutoConfig
import torch.nn as nn
import torch.nn.functional as F


# Calling the Tokenizer
checkpoint = "klue/roberta-base"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

class LanguageModel(nn.Module):
    def __init__(self, checkpoint):
        super(LanguageModel, self).__init__()

        # Load model with given checkpoint and get the body
        self.transformer = AutoModel.from_pretrained(checkpoint,
                                                   config=AutoConfig.from_pretrained(
                                                       checkpoint,
                                                       output_attentions=True, 
                                                       output_hidden_states=True))
        self.bidir_LSTM = nn.LSTM(768, 50, bidirectional=True) 
        self.flatten = nn.Flatten()
        self.dropout = nn.Dropout(p= 0.2)
        self.dense_50 = nn.Linear(100, 50)
    
    def forward(self, input_ids, attention_mask):
        # Extract outputs from the body
        input_ids = input_ids
        attention_mask = attention_mask
        outputs = self.transformer(input_ids, attention_mask)
        
        # Add custom layers
        LSTM_out, _ = self.bidir_LSTM(outputs.last_hidden_state)
        max_pool_out, _ = torch.max(LSTM_out, 1)
        output = F.relu(self.dense_50(max_pool_out))
        output = self.dropout(output)
        return output

class ImgModel(nn.Module):
    def __init__(self, num_classes):
        super(ImgModel, self).__init__()
        
        self.base_model = models.densenet121(pretrained=True)
        for param in self.base_model.parameters():
            param.requires_grad = True
        self.avg_pool = nn.AdaptiveAvgPool2d((1, 1))
        self.batch_norm1 = nn.BatchNorm2d(1024)
        self.Flatten = nn.Flatten()
        self.dropout1 = nn.Dropout(p=0.5)
        self.fc1 = nn.Linear(1024, 256)
        self.batch_norm2 = nn.BatchNorm1d(256)
        self.fc2 = nn.Linear(256, 50)
        self.batch_norm3 = nn.BatchNorm1d(50)
        self.fc3 = nn.Linear(50, 2)
        self.softmax= nn.Softmax(dim=1)
        
    def forward(self, img_input, label):
        x = img_input

        x = self.base_model.features(x)
        x = self.avg_pool(x)
        x = self.batch_norm1(x)
        x = self.Flatten(x)
        x = F.relu(self.fc1(x))
        x = self.batch_norm2(x)
        x = self.dropout1(x)
        x = F.relu(self.fc2(x))
        x = self.batch_norm3(x)
        x = self.fc3(x)
        x = self.softmax(x)
        return x

class ConcatModel(nn.Module):
    def __init__(self, checkpoint):
        super(ConcatModel, self).__init__()
        self.nlp_model = LanguageModel(checkpoint=checkpoint)
        self.img_model = ImgModel(num_classes=2)
        
        # Define dense layer with 20 units
        self.dense_layer = nn.Linear(100, 20)
        
        # Define dropout layer with 0.2 dropout rate
        self.dropout_layer = nn.Dropout(0.2)

        self.dense_layer2= nn.Linear(20,2)
        
        # Define final dense layer with softmax activation
        self.softmax_layer = nn.Softmax(dim=1)
        
    def forward(self, input_ids, attention_mask, img_input, label):
        nlp_output = self.nlp_model(input_ids, attention_mask)
        img_output = self.img_model(img_input)
        
        # Concatenate the outputs of the two models
        concat_output = torch.cat((nlp_output, img_output), dim=1)
        

        
        # Apply dense layer, dropout layer, and softmax layer
        dense_output = self.dense_layer(concat_output)
        dropout_output = self.dropout_layer(dense_output)
        dense_output2 = self.dense_layer2(dropout_output)
        softmax_output = self.softmax_layer(dense_output2)
        
        return softmax_output

# Save only the state_dict
torch.save(model.state_dict(), '/content/drive/MyDrive/Thumbnail.pt')

# Load only the state_dict
model = ConcatModel(checkpoint)
model.load_state_dict(torch.load('/content/drive/MyDrive/Thumbnail.pt', map_location=torch.device('cpu')))
model.eval()  # Set the model to evaluation mode


# Define the prediction function
@st.cache(allow_output_mutation=True)
def predict(image_path, title):
    # Preprocess the image and title
    image = Image.open(image_path).convert('RGB')
    transformed_image = transforms.Compose([transforms.Resize((180, 320)),
                                             transforms.ToTensor(),
                                             transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))
                                             ])(image).unsqueeze(0)
    inputs = tokenizer(title, truncation=True, max_length=100, add_special_tokens=True, padding='max_length')

    inputs_ids = torch.tensor(inputs["input_ids"]).unsqueeze(0)
    attention_mask = torch.tensor(inputs["attention_mask"]).unsqueeze(0)
    label = 1

    with torch.no_grad():
        output = model.forward(inputs_ids, attention_mask, transformed_image, label)

    probs = torch.nn.functional.softmax(output, dim=1)
    conf, classes = torch.max(probs, 1)
    
    # Define the class labels
    class_labels = {0: "good", 1: "bad"}
    
    return class_labels[classes.item()], conf.item()

# Define the Streamlit app
def main():
    st.title("Image and Text Classifier")

    # Create a file uploader for the image
    image_file = st.file_uploader("Upload an image", type=["jpg", "jpeg", "png"])

    # Create a text input box
    text_input = st.text_input("Enter some text")

    if image_file is not None and text_input != "":
        # Load the image from the file uploader
        image = Image.open(image_file)

        # Display the image
        st.image(image, caption="Uploaded Image", use_column_width=True)

        # Make a prediction with both image and text inputs
        prediction, confidence = predict(image_file, text_input)

        # Display the prediction
        st.write(f"Prediction: {prediction} with confidence score of {confidence:.2f}")

# Run the app
if __name__ == '__main__':
    main()

streamlit run /usr/local/lib/python3.8/dist-packages/ipykernel_launcher.py
